{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x,derivative=False):\n",
    "    if derivative:\n",
    "        np.where(x>0,1,0.)\n",
    "    return np.where(x>0,x,0.)\n",
    "\n",
    "def sigmoid(x,derivative=False):\n",
    "    f = 1/(1+np.exp(-x))\n",
    "    if derivative:\n",
    "        return f*(1-f)\n",
    "    return f\n",
    "\n",
    "def tanh(x,derivative=False):\n",
    "    f = (np.exp(2*x)-1)/(np.exp(2*x)+1)\n",
    "    if derivative:\n",
    "        return 1-f**2\n",
    "    return f\n",
    "\n",
    "def elu(x,derivative=False):\n",
    "    if derivative:\n",
    "        np.where(x>0,1,np.exp(x))\n",
    "    return np.where(x>0,x,np.exp(x)-1)\n",
    "\n",
    "def linear(x,derivative=False):\n",
    "    if derivative:\n",
    "        return sign(x)\n",
    "    return x\n",
    "\n",
    "def softmax(x,axis=-1):\n",
    "    shift_x = x - np.max(x,axis,keepdims=True)\n",
    "    return np.exp(shift_x)/np.sum(np.exp(shift_x),axis,keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(outputs,ground_truth,epsilon=1e-8):\n",
    "    return -np.mean(np.sum(ground_truth*np.log(outputs+epsilon),1))\n",
    "\n",
    "def mse(outputs,ground_truth):\n",
    "    return np.mean(0.5*np.sum((outputs-ground_truth)**2,1))\n",
    "\n",
    "def clip_by_global_norm(param_gradients,clip_value):\n",
    "    global_norm = np.sum([np.linalg.norm(grad) for grad in param_gradients])\n",
    "    return [grad*clip_value/max(clip_value,global_norm) for grad in param_gradients]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self,eta,beta1=0.9,beta2=0.999,epsilon=10e-8):\n",
    "        self.eta = eta\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "    def __call__(self,iteration,last_m,last_v,gradient):\n",
    "        adaptive_eta = self.eta*np.sqrt(1-self.beta2**iteration)/(1-self.beta1**iteration)\n",
    "        current_m = self.beta1*last_m+(1-self.beta1)*gradient\n",
    "        current_v = self.beta2*last_v+(1-self.beta2)*gradient**2\n",
    "        return -adaptive_eta*current_m/(np.sqrt(current_v)+self.epsilon),current_m,current_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell:\n",
    "    def __init__(self,num_features,state_size,output_size,random_seed):\n",
    "        self._num_features = num_features\n",
    "        self.state_size = state_size\n",
    "        self.output_size = output_size\n",
    "        self.params,self.moments = self.initialize_parameters(random_seed)\n",
    "    def initialize_parameters(self,random_seed):\n",
    "        np.random.seed(random_seed)\n",
    "        params = dict()\n",
    "        moments = dict()\n",
    "        for label in ['update','reset','hidden']:\n",
    "            params[label] = {'weights':np.random.randn(self._num_features+self.state_size,self.state_size)*0.1,\n",
    "                           'bias' : np.zeros((1,self.state_size))}\n",
    "            moments[label] = {'weights': [np.zeros((self._num_features+self.state_size,self.state_size))]*2,\n",
    "                           'bias' : [np.zeros((1,self.state_size))]*2}\n",
    "        params['output'] = {'weights':np.random.randn(self.state_size,self.output_size)*0.1,\n",
    "                           'bias' : np.zeros((1,self.output_size))}\n",
    "        moments['output'] = {'weights':[np.zeros((self.state_size,self.output_size))]*2,\n",
    "                           'bias' : [np.zeros((1,self.output_size))]*2}\n",
    "        np.random.seed(None)\n",
    "        return params,moments\n",
    "    def __call__(self,features,last_state,output_activation):\n",
    "        # features and last_state comes in batch x features\n",
    "        concat = np.hstack((features,last_state))\n",
    "        update = sigmoid(np.dot(concat,self.params['update']['weights'])+self.params['update']['bias'])\n",
    "        reset = sigmoid(np.dot(concat,self.params['reset']['weights'])+self.params['reset']['bias'])\n",
    "        hidden = tanh(np.dot(np.hstack((features,last_state*reset)),self.params['hidden']['weights'])+self.params['hidden']['bias'])\n",
    "        state = (1-update)*last_state+update*hidden\n",
    "        output = output_activation(np.dot(state,self.params['output']['weights'])+self.params['output']['bias'])\n",
    "        return update,reset,hidden,state,output\n",
    "    def backpropogate(self,ds_next,inpt,previous_state,hidden,reset,update):\n",
    "        \n",
    "        concat = np.hstack((inpt,previous_state))\n",
    "        \n",
    "        ds = ds_next\n",
    "        dh = ds * update * tanh(hidden,True)\n",
    "        \n",
    "        dWh = np.dot(np.hstack((inpt,previous_state*reset)).T,dh)\n",
    "        dbh = np.sum(dh,0)\n",
    "        \n",
    "        dri = np.dot(dh,self.params['hidden']['weights'][self._num_features:].T)\n",
    "        dr = previous_state * dri * sigmoid(reset,True)\n",
    "        \n",
    "        dWr = np.dot(concat.T,dr)\n",
    "        dbr = np.sum(dr,0)\n",
    "        \n",
    "        dz = (previous_state - hidden) * sigmoid(update,True)\n",
    "        \n",
    "        dWz = np.dot(concat.T,dz)\n",
    "        dbz = np.sum(dz,0)\n",
    "        \n",
    "        ds_next = np.dot(dz,self.params['update']['weights'][self._num_features:].T) +\\\n",
    "                    (1-update)*ds + dri*reset + np.dot(dr,self.params['reset']['weights'][self._num_features:].T)\n",
    "        return ds_next, dWh, dbh, dWr, dbr, dWz, dbz\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self,num_features,state_size,output_size,random_seed=None):\n",
    "        self.cell = GRUCell(num_features,state_size,output_size,random_seed)\n",
    "    def predict(self,features,initial_state=None):\n",
    "        # features come in as batch x seqlen x features\n",
    "        batch_size, seqlen = features.shape[:2]\n",
    "        \n",
    "        resets = np.zeros((batch_size,seqlen,self.cell.state_size))\n",
    "        updates = np.zeros((batch_size,seqlen,self.cell.state_size))\n",
    "        hiddens = np.zeros((batch_size,seqlen,self.cell.state_size))\n",
    "        states = np.zeros((batch_size,seqlen+1,self.cell.state_size))\n",
    "        outputs = np.zeros((batch_size,seqlen,self.cell.output_size))\n",
    "        \n",
    "        if initial_state is not None:\n",
    "            states[:,0,:] = initial_state\n",
    "            \n",
    "        for t in range(1,seqlen+1):\n",
    "            updates[:,t-1,:],resets[:,t-1,:],hiddens[:,t-1,:],states[:,t,:],outputs[:,t-1,:] = self.cell(features[:,t-1,:],states[:,t-1,:],linear)\n",
    "        return outputs,states,hiddens,resets,updates\n",
    "    \n",
    "    def backpropogate(self,iteration,adam,cost_function,outputs,labels,features,states,hiddens,resets,updates):\n",
    "        batch_size,seqlen,_ = features.shape\n",
    "        \n",
    "        loss = cost_function(outputs[:,seqlen-1,:],labels)\n",
    "        dloss = (outputs[:,seqlen-1,:] - labels)/batch_size\n",
    "        \n",
    "        dWo = np.dot(states[:,seqlen,:].T,dloss)\n",
    "        dbo = np.sum(dloss,0)\n",
    "        \n",
    "        ds_next = np.dot(dloss,self.cell.params['output']['weights'].T)\n",
    "        AdWo, Adbo, AdWh, Adbh, AdWr, Adbr, AdWz, Adbz = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "        for t in reversed(range(1,seqlen+1)):\n",
    "            \n",
    "            ds_next, dWh, dbh, dWr, dbr, dWz, dbz = self.cell.backpropogate(\n",
    "                                                        ds_next,features[:,t-1,:],states[:,t-1,:],hiddens[:,t-1,:],\n",
    "                                                        resets[:,t-1,:],updates[:,t-1,:])\n",
    "            AdWo+=dWo; Adbo += dbo; AdWh += dWh; Adbh += dbh; AdWr += dWr; Adbr += dbr; AdWz += dWz; Adbz += dbz\n",
    "        dWo, dbo, AdWh, Adbh, AdWr, Adbr, AdWz, Adbz = clip_by_global_norm([dWo, dbo, AdWh, Adbh, AdWr, Adbr, AdWz, Adbz],10)\n",
    "        #outputs\n",
    "        m, v = self.cell.moments['output']['weights']\n",
    "        update, m, v = adam(iteration,m,v,dWo)\n",
    "        self.cell.moments['output']['weights'] = [m,v]\n",
    "        self.cell.params['output']['weights'] = self.cell.params['output']['weights']+update\n",
    "        m, v = self.cell.moments['output']['bias']\n",
    "        update, m, v = adam(iteration,m,v,dbo)\n",
    "        self.cell.moments['output']['bias'] = [m,v]\n",
    "        self.cell.params['output']['bias'] = self.cell.params['output']['bias']+update\n",
    "        \n",
    "        #update\n",
    "        m, v = self.cell.moments['update']['weights']\n",
    "        update, m, v = adam(iteration,m,v,AdWz)\n",
    "        self.cell.moments['update']['weights'] = [m,v]\n",
    "        self.cell.params['update']['weights'] = self.cell.params['update']['weights']+update\n",
    "        m, v = self.cell.moments['update']['bias']\n",
    "        update, m, v = adam(iteration,m,v,Adbz)\n",
    "        self.cell.moments['update']['bias'] = [m,v]\n",
    "        self.cell.params['update']['bias'] = self.cell.params['update']['bias']+update\n",
    "        \n",
    "        #reset\n",
    "        m, v = self.cell.moments['reset']['weights']\n",
    "        update, m, v = adam(iteration,m,v,AdWr)\n",
    "        self.cell.moments['reset']['weights'] = [m,v]\n",
    "        self.cell.params['reset']['weights'] = self.cell.params['reset']['weights']+update\n",
    "        m, v = self.cell.moments['reset']['bias']\n",
    "        update, m, v = adam(iteration,m,v,Adbr)\n",
    "        self.cell.moments['reset']['bias'] = [m,v]\n",
    "        self.cell.params['reset']['bias'] = self.cell.params['reset']['bias']+update\n",
    "        \n",
    "        #hidden\n",
    "        m, v = self.cell.moments['hidden']['weights']\n",
    "        update, m, v = adam(iteration,m,v,AdWh)\n",
    "        self.cell.moments['hidden']['weights'] = [m,v]\n",
    "        self.cell.params['hidden']['weights'] = self.cell.params['hidden']['weights']+update\n",
    "        m, v = self.cell.moments['hidden']['bias']\n",
    "        update, m, v = adam(iteration,m,v,Adbh)\n",
    "        self.cell.moments['hidden']['bias'] = [m,v]\n",
    "        self.cell.params['hidden']['bias'] = self.cell.params['hidden']['bias']+update\n",
    "        \n",
    "        return loss\n",
    "          \n",
    "    def train(self,train_inputs,seqlen,batch_size,epochs,lr,cost_function):\n",
    "        adam = Adam(lr)\n",
    "        \n",
    "        sinputs = np.array([train_inputs[t-seqlen+1:t+1,:] for t in range(seqlen-1,train_inputs.shape[0]-1)])\n",
    "        stargets = train_inputs[seqlen:,:]\n",
    "        \n",
    "        num_datum = sinputs.shape[0]\n",
    "        \n",
    "        iteration = 1\n",
    "        \n",
    "        losses = []\n",
    "        for i in range(epochs):\n",
    "            batch_num = 0\n",
    "            random_indices = np.random.permutation(num_datum)\n",
    "            while batch_num < num_datum:\n",
    "                if batch_num + 2*batch_size > num_datum:\n",
    "                    current_batch = random_indices[batch_num:]\n",
    "                else:\n",
    "                    current_batch = random_indices[batch_num:(batch_num+batch_size)]\n",
    "                x_batch, y_batch = sinputs[current_batch,:,:],stargets[current_batch,:]\n",
    "                \n",
    "                #forward_pass\n",
    "                outputs,states,hiddens,resets,updates = self.predict(x_batch)\n",
    "                \n",
    "                #backprop\n",
    "                #print(outputs)\n",
    "                losses.append(self.backpropogate(iteration,adam,mse,outputs,y_batch,x_batch,states,hiddens,resets,updates))\n",
    "                \n",
    "                batch_num += batch_size\n",
    "                iteration+=1\n",
    "        return losses\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
