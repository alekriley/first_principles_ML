{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self,eta,beta1=0.9,beta2=0.999,epsilon=10e-8):\n",
    "        self.eta = eta\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "    def __call__(self,iteration,last_m,last_v,gradient):\n",
    "        adaptive_eta = self.eta*np.sqrt(1-self.beta2**iteration)/(1-self.beta1**iteration)\n",
    "        current_m = self.beta1*last_m+(1-self.beta1)*gradient\n",
    "        current_v = self.beta2*last_v+(1-self.beta2)*gradient**2\n",
    "        return -adaptive_eta*current_m/(np.sqrt(current_v)+self.epsilon),current_m,current_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x,derivative=False):\n",
    "    if derivative:\n",
    "        np.where(x>0,1,0.)\n",
    "    return np.where(x>0,x,0.)\n",
    "\n",
    "def sigmoid(x,derivative=False):\n",
    "    f = 1/(1+np.exp(-x))\n",
    "    if derivative:\n",
    "        return f*(1-f)\n",
    "    return f\n",
    "\n",
    "def tanh(x,derivative=False):\n",
    "    f = (np.exp(2*x)-1)/(np.exp(2*x)+1)\n",
    "    if derivative:\n",
    "        return 1-f**2\n",
    "    return f\n",
    "\n",
    "def elu(x,derivative=False):\n",
    "    if derivative:\n",
    "        np.where(x>0,1,np.exp(x))\n",
    "    return np.where(x>0,x,np.exp(x)-1)\n",
    "\n",
    "def linear(x,derivative=False):\n",
    "    if derivative:\n",
    "        return sign(x)\n",
    "    return x\n",
    "\n",
    "def softmax(x,axis=-1):\n",
    "    shift_x = x - np.max(x,axis,keepdims=True)\n",
    "    return np.exp(shift_x)/np.sum(np.exp(shift_x),axis,keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(outputs,ground_truth,epsilon=1e-8):\n",
    "    return -np.mean(np.sum(ground_truth*np.log(outputs+epsilon),1))\n",
    "\n",
    "def mse(outputs,ground_truth):\n",
    "    return np.mean(0.5*np.sum((outputs-ground_truth)**2,1))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,num_features,neurons_per_layer,activation_per_layer,random_seed=None):\n",
    "        self._num_features = num_features\n",
    "        self._neurons = neurons_per_layer\n",
    "        self._num_layers = len(self._neurons)\n",
    "        self.activations = activation_per_layer\n",
    "        self.params,self.moments = self.initialize_parameters(random_seed)\n",
    "    def initialize_parameters(self,random_seed):\n",
    "        np.random.seed(random_seed)\n",
    "        parameters = []\n",
    "        moments = []\n",
    "        input_neurons = self._num_features\n",
    "        for i,output_neurons in enumerate(self._neurons):\n",
    "            parameters.append({\"weights\":np.random.randn(input_neurons,output_neurons)*0.1,\n",
    "                                                 \"bias\": np.zeros((1,output_neurons))})\n",
    "            moments.append({\"weights\":[np.zeros([input_neurons,output_neurons])]*2,\n",
    "                                                 \"bias\": [np.zeros((1,output_neurons))]*2})\n",
    "            input_neurons = output_neurons\n",
    "        np.random.seed(None)\n",
    "        return parameters,moments\n",
    "    def predict(self,features):\n",
    "        outputs = [features]\n",
    "        for layer,activation in zip(self.params,self.activations):\n",
    "            outputs.append(np.atleast_2d(activation(np.dot(outputs[-1],layer['weights'])+layer['bias'])))\n",
    "        return outputs\n",
    "    def backprop(self,outputs,labels,adam,iteration,cost_function):\n",
    "        loss = cost_function(outputs[self._num_layers],labels)\n",
    "        bp_error = (outputs[self._num_layers]-labels)/labels.shape[0]\n",
    "        for i in reversed(range(0,self._num_layers)):\n",
    "            weights = self.params[i]['weights']\n",
    "            bias = self.params[i]['bias']\n",
    "            \n",
    "            #calculate gradients and error to be backpropogated\n",
    "            weight_grad = outputs[i].T@bp_error\n",
    "            bias_grad = np.sum(bp_error,0)\n",
    "            if i > 0:\n",
    "                bp_error = bp_error@weights.T*self.activations[i-1](outputs[i],True)\n",
    "            \n",
    "            #update parameters and moment vectors\n",
    "            wm,wv = self.moments[i]['weights']\n",
    "            bm,bv = self.moments[i]['bias']\n",
    "            wupdate,wm,wv = adam(iteration,wm,wv,weight_grad)\n",
    "            bupdate,bm,bv = adam(iteration,bm,bv,bias_grad)\n",
    "            \n",
    "            self.moments[i]['weights'] = [wm,wv]\n",
    "            self.moments[i]['bias'] = [bm,bv]\n",
    "            self.params[i]['weights'] = weights+wupdate\n",
    "            self.params[i]['bias'] = bias+bupdate\n",
    "        return loss\n",
    "    def train(self,train_inputs,train_target,batch_size,epochs,lr,cost_function):\n",
    "        adam = Adam(lr)\n",
    "        \n",
    "        num_datum,num_features = train_inputs.shape\n",
    "        iteration = 1\n",
    "        \n",
    "        losses = []\n",
    "        for i in range(epochs):\n",
    "            batch_num = 0\n",
    "            random_indices = np.random.permutation(num_datum)\n",
    "            while batch_num < num_datum:\n",
    "                if batch_num + 2*batch_size > num_datum:\n",
    "                    current_batch = random_indices[batch_num:]\n",
    "                else:\n",
    "                    current_batch = random_indices[batch_num:(batch_num+batch_size)]\n",
    "                x_batch, y_batch = train_inputs[current_batch,:],np.atleast_2d(train_target[current_batch,:])\n",
    "                \n",
    "                #forward_pass\n",
    "                outputs = self.predict(x_batch)\n",
    "                \n",
    "                #backprop\n",
    "                losses.append(self.backprop(outputs,y_batch,adam,iteration,cost_function))\n",
    "                \n",
    "                batch_num += batch_size\n",
    "                iteration+=1 \n",
    "        return losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
